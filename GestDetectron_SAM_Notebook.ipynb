{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the Workspace\n",
        "\n",
        "To run this jupyter notebook, you will require test.mp4 and the requirements.txt from this [github repo](https://github.com/therealnaveenkamal/gest_detectron). Make sure you have python3.12 installed."
      ],
      "metadata": {
        "id": "7ouIAj8DX9qT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -r requirements.txt\n",
        "\n",
        "!pip install mediapipe\n",
        "!pip install opencv-python\n",
        "!pip install matplotlib\n",
        "!pip install \"git+https://github.com/facebookresearch/sam2.git\"\n",
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "#If you run the notebook in colab, you might encounter a prompt asing you to restart due to package import. Feel free to click CANCEL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "sIqy5KAURaLI",
        "outputId": "e1ab0872-5e4a-4e95-9f39-26b1c33a49e1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.20-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (24.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.21)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.5)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.55.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.20-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.20 sounddevice-0.5.1\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting git+https://github.com/facebookresearch/sam2.git\n",
            "  Cloning https://github.com/facebookresearch/sam2.git to /tmp/pip-req-build-xuyh1xio\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/sam2.git /tmp/pip-req-build-xuyh1xio\n",
            "  Resolved https://github.com/facebookresearch/sam2.git to commit 2b90b9f5ceec907a1c18123530e92e794ad901a4\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.20.1 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (4.67.1)\n",
            "Collecting hydra-core>=1.3.2 (from SAM-2==1.0)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting iopath>=0.1.10 (from SAM-2==1.0)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (11.1.0)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.3.2->SAM-2==1.0)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.3.2->SAM-2==1.0)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (24.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.10->SAM-2==1.0) (4.12.2)\n",
            "Collecting portalocker (from iopath>=0.1.10->SAM-2==1.0)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.5.1->SAM-2==1.0) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.5.1->SAM-2==1.0) (1.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.3.2->SAM-2==1.0) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.5.1->SAM-2==1.0) (3.0.2)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: SAM-2, antlr4-python3-runtime, iopath\n",
            "  Building wheel for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SAM-2: filename=SAM_2-1.0-cp311-cp311-linux_x86_64.whl size=523635 sha256=ea5e68288a78a6eae71809c834364258d532fddd74512c9d341d315ae4862619\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9po2u2ek/wheels/d8/63/41/d37b316a85599f58a42be0210805ecf8594b9c06082028716e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=e7d15893824a76692f17aa824cc1ba41762584ececc0799b70eaa11452e9d2c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=db152a40227c656cc6fc265ed8240a6fffefd8f54264de79d419928b29eb52dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built SAM-2 antlr4-python3-runtime iopath\n",
            "Installing collected packages: antlr4-python3-runtime, portalocker, omegaconf, iopath, hydra-core, SAM-2\n",
            "Successfully installed SAM-2-1.0 antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 iopath-0.1.10 omegaconf-2.3.0 portalocker-3.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "3d6148e88d7b419baa567fa492aa5e11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
        "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
      ],
      "metadata": {
        "id": "_sGxqK3LOGX2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Required Modules"
      ],
      "metadata": {
        "id": "lSR3bI8nXYnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import mediapipe as mp\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OkQRZmiWxDN",
        "outputId": "54634733-2c9f-421d-9754-9e89c896d841"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.1+cu121\n",
            "Torchvision version: 0.20.1+cu121\n",
            "CUDA is available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frame Extraction and Hand Landmarker data extraction using MediaPipe"
      ],
      "metadata": {
        "id": "POcvF0_iXX9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MARGIN = 10  # pixels\n",
        "FONT_SIZE = 1\n",
        "FONT_THICKNESS = 1\n",
        "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "  hand_landmarks_list = detection_result.hand_landmarks\n",
        "  handedness_list = detection_result.handedness\n",
        "  annotated_image = np.copy(rgb_image)\n",
        "\n",
        "  # Loop through the detected hands to visualize.\n",
        "  for idx in range(len(hand_landmarks_list)):\n",
        "    hand_landmarks = hand_landmarks_list[idx]\n",
        "    handedness = handedness_list[idx]\n",
        "\n",
        "    # Draw the hand landmarks.\n",
        "    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "    hand_landmarks_proto.landmark.extend([\n",
        "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
        "    ])\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "      annotated_image,\n",
        "      hand_landmarks_proto,\n",
        "      solutions.hands.HAND_CONNECTIONS,\n",
        "      solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
        "      solutions.drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "    # Get the top left corner of the detected hand's bounding box.\n",
        "    height, width, _ = annotated_image.shape\n",
        "    x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
        "    y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
        "    text_x = int(min(x_coordinates) * width)\n",
        "    text_y = int(min(y_coordinates) * height) - MARGIN\n",
        "\n",
        "    # Draw handedness (left or right hand) on the image.\n",
        "    cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
        "                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
        "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
        "\n",
        "  return annotated_image\n",
        "\n",
        "\n",
        "# MediaPipe setup\n",
        "BaseOptions = mp.tasks.BaseOptions\n",
        "HandLandmarker = mp.tasks.vision.HandLandmarker\n",
        "HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\n",
        "VisionRunningMode = mp.tasks.vision.RunningMode\n",
        "\n",
        "# Configure video processing options\n",
        "options = HandLandmarkerOptions(\n",
        "    base_options=BaseOptions(model_asset_path='hand_landmarker.task'),\n",
        "    running_mode=VisionRunningMode.VIDEO,  # VIDEO mode\n",
        "    num_hands=2)\n",
        "\n",
        "def process_video(input_path, output_path):\n",
        "    # Initialize video capture\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "    # Get video properties\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Initialize video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    frame_data = []\n",
        "\n",
        "    with HandLandmarker.create_from_options(options) as detector:\n",
        "        frame_timestamp = 0\n",
        "        count = 0\n",
        "        print(\"Hand Landmarking In Progress...\")\n",
        "\n",
        "        output_dir = \"frames\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            if not ret:\n",
        "                print(\"Hand Landmarked Video Rendering Completed\")\n",
        "                break\n",
        "\n",
        "            # Convert BGR to RGB\n",
        "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Create MediaPipe Image\n",
        "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
        "\n",
        "            # Detect hand landmarks (with timestamp)\n",
        "            detection_result = detector.detect_for_video(mp_image, frame_timestamp)\n",
        "\n",
        "            frame_entry = {\n",
        "                \"timestamp\": frame_timestamp,\n",
        "                \"landmarks\": [],\n",
        "                \"handedness\": []\n",
        "            }\n",
        "\n",
        "            for hand_landmarks in detection_result.hand_landmarks:\n",
        "                frame_entry[\"landmarks\"].append([(lm.x, lm.y) for lm in hand_landmarks])\n",
        "\n",
        "            for classification in detection_result.handedness:\n",
        "                frame_entry[\"handedness\"].append([(c.category_name, c.score) for c in classification])\n",
        "\n",
        "            frame_data.append(frame_entry)\n",
        "\n",
        "            # Draw landmarks (using your existing function)\n",
        "            annotated_image = draw_landmarks_on_image(rgb_frame, detection_result)\n",
        "\n",
        "            # Convert back to BGR for video output\n",
        "            bgr_frame = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            # Write processed frame\n",
        "            out.write(bgr_frame)\n",
        "\n",
        "            output_file = os.path.join(output_dir, f\"{count:05d}.jpg\")\n",
        "            # Save the frame as a JPG with specified quality\n",
        "            cv2.imwrite(output_file, frame, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
        "\n",
        "            # Increment frame timestamp on milliseconds\n",
        "            frame_timestamp += int(1000 / fps)\n",
        "            count+=1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    return frame_data"
      ],
      "metadata": {
        "id": "18OevTpfWrqZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions for SAM2"
      ],
      "metadata": {
        "id": "oawGjEXHXFUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        cmap = plt.get_cmap(\"tab10\")\n",
        "        if(obj_id<=20):\n",
        "          cmap_idx = 1\n",
        "        else:\n",
        "          cmap_idx = 2\n",
        "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=20):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='.', s=marker_size, edgecolor='white', linewidth=1)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='.', s=marker_size, edgecolor='white', linewidth=1)\n",
        "\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))"
      ],
      "metadata": {
        "id": "rzAhTaaKW3l1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictor Initialization - Feeding Clicks from Hand Landmarker data"
      ],
      "metadata": {
        "id": "KqbtYDlJXWhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_sam_predictor(fd, device, sam2_checkpoint = \"sam2.1_hiera_large.pt\", model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\", video_dir = \"./frames\"):\n",
        "    predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device)\n",
        "    inference_state = predictor.init_state(video_path=video_dir)\n",
        "\n",
        "    predictor.reset_state(inference_state)\n",
        "    prompts={}\n",
        "\n",
        "    cap = cv2.VideoCapture(\"test.mp4\")\n",
        "\n",
        "    # Get video properties\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "\n",
        "    ann_frame_idx = 0  # the frame index we interact with\n",
        "    ic, ic2 = 0, 0\n",
        "\n",
        "    for ic, elem in enumerate(fd[0]['landmarks'][0]):\n",
        "        if(ic%4 ==0):\n",
        "            temp = []\n",
        "            ann_obj_id = ic\n",
        "            temp.append([elem[0]*width, elem[1]*height])\n",
        "            points = np.array(temp, dtype=np.float32)\n",
        "            labels = np.array(np.ones(1), np.int32)\n",
        "\n",
        "            prompts[ann_obj_id] = points, labels\n",
        "\n",
        "\n",
        "            _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
        "                inference_state=inference_state,\n",
        "                frame_idx=ann_frame_idx,\n",
        "                obj_id=ann_obj_id,\n",
        "                points=points,\n",
        "                labels=labels,\n",
        "            )\n",
        "\n",
        "    ic+=1\n",
        "    for ic2, elem in enumerate(fd[0]['landmarks'][1]):\n",
        "        if(ic2%4 ==0):\n",
        "            temp = []\n",
        "            ann_obj_id = ic+ic2\n",
        "            temp.append([elem[0]*width, elem[1]*height])\n",
        "            points = np.array(temp, dtype=np.float32)\n",
        "            labels = np.array(np.ones(1), np.int32)\n",
        "\n",
        "            prompts[ann_obj_id] = points, labels\n",
        "\n",
        "\n",
        "            _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
        "                inference_state=inference_state,\n",
        "                frame_idx=ann_frame_idx,\n",
        "                obj_id=ann_obj_id,\n",
        "                points=points,\n",
        "                labels=labels,\n",
        "            )\n",
        "\n",
        "    return predictor, inference_state\n"
      ],
      "metadata": {
        "id": "ER9YWanOXLXK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Render Final SAM2 Segmented Video Output"
      ],
      "metadata": {
        "id": "8yTyAmYTXU4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def render_sam_video(video_path, video_segments, output_path, alpha=0.5):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Create color mapping for objects\n",
        "    obj_ids = list({k for frame in video_segments.values() for k in frame.keys()})\n",
        "\n",
        "    frame_idx = 0\n",
        "\n",
        "    output_dir = \"segments\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_idx in video_segments:\n",
        "            # Convert to RGB for processing (SAM masks are in RGB space)\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            overlay = frame_rgb.copy()\n",
        "\n",
        "            for obj_id, mask in video_segments[frame_idx].items():\n",
        "                # Ensure mask is 2D and matches frame dimensions\n",
        "                mask = mask[0]\n",
        "                if mask.shape != (height, width):\n",
        "                    mask = cv2.resize(mask.astype(np.uint8), (width, height))\n",
        "\n",
        "                cmap = plt.get_cmap(\"tab10\")\n",
        "\n",
        "                if(obj_id <=20):\n",
        "                  color = [255, 0, 0]\n",
        "                else:\n",
        "                  color = [0, 255, 0]\n",
        "\n",
        "                # Create colored mask\n",
        "                mask_bgr = np.zeros_like(overlay)\n",
        "                mask_bgr[mask] = color\n",
        "\n",
        "                # Blend mask with overlay\n",
        "                overlay = cv2.addWeighted(overlay, 1, mask_bgr, alpha, 0)\n",
        "\n",
        "            # Convert back to BGR for video writing\n",
        "            frame_out = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)\n",
        "            output_file = os.path.join(output_dir, f\"{frame_idx:05d}.jpg\")\n",
        "            # Save the frame as a JPG with specified quality\n",
        "            cv2.imwrite(output_file, frame_out, [int(cv2.IMWRITE_JPEG_QUALITY), 50])\n",
        "\n",
        "            out.write(frame_out)\n",
        "\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Saved SAM masked video to {output_path}\")"
      ],
      "metadata": {
        "id": "9tn10V69XOSL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Code - Execution**"
      ],
      "metadata": {
        "id": "6HO8pNiKXVRu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wf-koa6NPJn",
        "outputId": "e7779f87-64f6-4d4d-9297-53aff05b40da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hand Landmarking In Progress...\n",
            "Hand Landmarked Video Rendering Completed\n",
            "Video Frame Extracted\n",
            "using device: cuda\n",
            "Predictor Calling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "frame loading (JPEG): 100%|██████████| 210/210 [00:09<00:00, 21.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictor Initialized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "propagate in video: 100%|██████████| 210/210 [14:00<00:00,  4.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved SAM masked video to sam_masked_output_final.mp4\n"
          ]
        }
      ],
      "source": [
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "# Generating Hand Landmarks\n",
        "\n",
        "frame_data = process_video(\"test.mp4\", \"landmarked_output.mp4\")\n",
        "\n",
        "print(\"Video Frame Extracted\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "    if torch.cuda.get_device_properties(0).major >= 8:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "elif device.type == \"mps\":\n",
        "    print(\n",
        "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
        "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
        "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
        "    )\n",
        "\n",
        "print(\"Predictor Calling\")\n",
        "\n",
        "predictor, inference_state = init_sam_predictor(frame_data, device, sam2_checkpoint = \"sam2.1_hiera_large.pt\", model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\", video_dir = \"./frames\")\n",
        "\n",
        "print(\"Predictor Initialized\")\n",
        "\n",
        "video_segments = {}\n",
        "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
        "    video_segments[out_frame_idx] = {\n",
        "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
        "        for i, out_obj_id in enumerate(out_obj_ids)\n",
        "    }\n",
        "\n",
        "\n",
        "render_sam_video(\n",
        "    video_path=\"test.mp4\",\n",
        "    video_segments=video_segments,\n",
        "    output_path=\"sam_masked_output_final.mp4\",\n",
        "    alpha=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dx5XWGI8O4Ua"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}