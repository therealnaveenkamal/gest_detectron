{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the Workspace\n",
        "\n",
        "To run this jupyter notebook, you will require test.mp4 and the requirements.txt from this [github repo](https://github.com/therealnaveenkamal/gest_detectron). Make sure you have python3.12 installed."
      ],
      "metadata": {
        "id": "7ouIAj8DX9qT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -r requirements.txt\n",
        "\n",
        "!pip install mediapipe\n",
        "!pip install opencv-python\n",
        "!pip install matplotlib\n",
        "!pip install \"git+https://github.com/facebookresearch/sam2.git\"\n",
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install gdown\n",
        "\n",
        "#If you run the notebook in colab, you might encounter a prompt asking you to restart due to package import. Feel free to click CANCEL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "sIqy5KAURaLI",
        "outputId": "6fde4a5e-a971-4a22-b4a2-0b58ec4b57fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.20-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.24)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.20-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.20 sounddevice-0.5.1\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting git+https://github.com/facebookresearch/sam2.git\n",
            "  Cloning https://github.com/facebookresearch/sam2.git to /tmp/pip-req-build-0a7scblv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/sam2.git /tmp/pip-req-build-0a7scblv\n",
            "  Resolved https://github.com/facebookresearch/sam2.git to commit 2b90b9f5ceec907a1c18123530e92e794ad901a4\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.20.1 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (0.20.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (4.67.1)\n",
            "Collecting hydra-core>=1.3.2 (from SAM-2==1.0)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting iopath>=0.1.10 (from SAM-2==1.0)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (11.1.0)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.3.2->SAM-2==1.0)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.3.2->SAM-2==1.0)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (24.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.10->SAM-2==1.0) (4.12.2)\n",
            "Collecting portalocker (from iopath>=0.1.10->SAM-2==1.0)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.5.1->SAM-2==1.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.5.1->SAM-2==1.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.5.1->SAM-2==1.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.5.1->SAM-2==1.0)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.5.1->SAM-2==1.0)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.5.1->SAM-2==1.0)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.5.1->SAM-2==1.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.5.1->SAM-2==1.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.5.1->SAM-2==1.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.5.1->SAM-2==1.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.5.1->SAM-2==1.0) (1.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.3.2->SAM-2==1.0) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.5.1->SAM-2==1.0) (3.0.2)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: SAM-2, antlr4-python3-runtime, iopath\n",
            "  Building wheel for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SAM-2: filename=SAM_2-1.0-cp311-cp311-linux_x86_64.whl size=470989 sha256=95d5b2e4a22cc78418982f6e4b4ba2a7fd3f52241d4007ac2e27eed585f72fbd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-smzh3tah/wheels/d8/63/41/d37b316a85599f58a42be0210805ecf8594b9c06082028716e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=b27073083a1fd67f409022d28ea7650bcdcb60f6f3d81d0d4fb786139c4c8d38\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=93f75576d11a7ed711797fdd8e464674c163c12e38b5b214ed1f2f799e2ff8b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built SAM-2 antlr4-python3-runtime iopath\n",
            "Installing collected packages: antlr4-python3-runtime, portalocker, omegaconf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, iopath, hydra-core, nvidia-cusolver-cu12, SAM-2\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed SAM-2-1.0 antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 iopath-0.1.10 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 omegaconf-2.3.0 portalocker-3.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "658a5ae251c04af78ea899a9908c457d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
        "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
      ],
      "metadata": {
        "id": "_sGxqK3LOGX2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1hx4sw_pYh_4L9yZdfEXao9Rb_I-nX-TC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rp80VqTCocy",
        "outputId": "3e68053d-69f1-4bd4-a527-eb4438e061ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1hx4sw_pYh_4L9yZdfEXao9Rb_I-nX-TC\n",
            "From (redirected): https://drive.google.com/uc?id=1hx4sw_pYh_4L9yZdfEXao9Rb_I-nX-TC&confirm=t&uuid=5ac3f482-4e75-4fca-b67f-5d5f305e8726\n",
            "To: /content/dinov2_hand_trained_segmentation.pth\n",
            "100% 355M/355M [00:03<00:00, 95.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Required Modules"
      ],
      "metadata": {
        "id": "lSR3bI8nXYnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import mediapipe as mp\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50\n",
        "import torch.hub\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OkQRZmiWxDN",
        "outputId": "41b5d751-cb4c-4ac2-b055-f80d03be3c6a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.1+cu124\n",
            "Torchvision version: 0.20.1+cu124\n",
            "CUDA is available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frame Extraction and Hand Landmarker data extraction using MediaPipe"
      ],
      "metadata": {
        "id": "POcvF0_iXX9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MARGIN = 10  # pixels\n",
        "FONT_SIZE = 1\n",
        "FONT_THICKNESS = 1\n",
        "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "  hand_landmarks_list = detection_result.hand_landmarks\n",
        "  handedness_list = detection_result.handedness\n",
        "  annotated_image = np.copy(rgb_image)\n",
        "\n",
        "  # Loop through the detected hands to visualize.\n",
        "  for idx in range(len(hand_landmarks_list)):\n",
        "    hand_landmarks = hand_landmarks_list[idx]\n",
        "    handedness = handedness_list[idx]\n",
        "\n",
        "    # Draw the hand landmarks.\n",
        "    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "    hand_landmarks_proto.landmark.extend([\n",
        "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
        "    ])\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "      annotated_image,\n",
        "      hand_landmarks_proto,\n",
        "      solutions.hands.HAND_CONNECTIONS,\n",
        "      solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
        "      solutions.drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "    # Get the top left corner of the detected hand's bounding box.\n",
        "    height, width, _ = annotated_image.shape\n",
        "    x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
        "    y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
        "    text_x = int(min(x_coordinates) * width)\n",
        "    text_y = int(min(y_coordinates) * height) - MARGIN\n",
        "\n",
        "    # Draw handedness (left or right hand) on the image.\n",
        "    cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
        "                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
        "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
        "\n",
        "  return annotated_image\n",
        "\n",
        "\n",
        "# MediaPipe setup\n",
        "BaseOptions = mp.tasks.BaseOptions\n",
        "HandLandmarker = mp.tasks.vision.HandLandmarker\n",
        "HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\n",
        "VisionRunningMode = mp.tasks.vision.RunningMode\n",
        "\n",
        "# Configure video processing options\n",
        "options = HandLandmarkerOptions(\n",
        "    base_options=BaseOptions(model_asset_path='hand_landmarker.task'),\n",
        "    running_mode=VisionRunningMode.VIDEO,  # VIDEO mode\n",
        "    num_hands=2)\n",
        "\n",
        "def process_video(input_path, output_path):\n",
        "    # Initialize video capture\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "    # Get video properties\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Initialize video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    frame_data = []\n",
        "\n",
        "    with HandLandmarker.create_from_options(options) as detector:\n",
        "        frame_timestamp = 0\n",
        "        count = 0\n",
        "        print(\"Hand Landmarking In Progress...\")\n",
        "\n",
        "        output_dir = \"frames\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            if not ret:\n",
        "                print(\"Hand Landmarked Video Rendering Completed\")\n",
        "                break\n",
        "\n",
        "            # Convert BGR to RGB\n",
        "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Create MediaPipe Image\n",
        "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
        "\n",
        "            # Detect hand landmarks (with timestamp)\n",
        "            detection_result = detector.detect_for_video(mp_image, frame_timestamp)\n",
        "\n",
        "            frame_entry = {\n",
        "                \"timestamp\": frame_timestamp,\n",
        "                \"landmarks\": [],\n",
        "                \"handedness\": []\n",
        "            }\n",
        "\n",
        "            for hand_landmarks in detection_result.hand_landmarks:\n",
        "                frame_entry[\"landmarks\"].append([(lm.x, lm.y) for lm in hand_landmarks])\n",
        "\n",
        "            for classification in detection_result.handedness:\n",
        "                frame_entry[\"handedness\"].append([(c.category_name, c.score) for c in classification])\n",
        "\n",
        "            frame_data.append(frame_entry)\n",
        "\n",
        "            # Draw landmarks (using your existing function)\n",
        "            annotated_image = draw_landmarks_on_image(rgb_frame, detection_result)\n",
        "\n",
        "            # Convert back to BGR for video output\n",
        "            bgr_frame = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            # Write processed frame\n",
        "            out.write(bgr_frame)\n",
        "\n",
        "            output_file = os.path.join(output_dir, f\"{count:05d}.jpg\")\n",
        "            # Save the frame as a JPG with specified quality\n",
        "            cv2.imwrite(output_file, frame, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
        "\n",
        "            # Increment frame timestamp on milliseconds\n",
        "            frame_timestamp += int(1000 / fps)\n",
        "            count+=1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    return frame_data"
      ],
      "metadata": {
        "id": "18OevTpfWrqZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions for SAM2"
      ],
      "metadata": {
        "id": "oawGjEXHXFUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        cmap = plt.get_cmap(\"tab10\")\n",
        "        cmap_idx = 0 if obj_id is None else obj_id\n",
        "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    print(h,w)\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=20):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='.', s=marker_size, edgecolor='white', linewidth=1)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='.', s=marker_size, edgecolor='white', linewidth=1)\n",
        "\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))"
      ],
      "metadata": {
        "id": "rzAhTaaKW3l1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictor Initialization - Feeding Clicks from Hand Landmarker data"
      ],
      "metadata": {
        "id": "KqbtYDlJXWhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_sam_predictor(model, device, sam2_checkpoint = \"sam2.1_hiera_large.pt\", model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\", video_dir = \"./frames\"):\n",
        "    predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device)\n",
        "    inference_state = predictor.init_state(video_path=video_dir)\n",
        "\n",
        "    predictor.reset_state(inference_state)\n",
        "    prompts={}\n",
        "\n",
        "    first_frame_path = os.path.join(video_dir, \"00000.jpg\")\n",
        "    _, mask_np = predict_mask(model, first_frame_path)\n",
        "\n",
        "\n",
        "    y, x = np.where(np.array(mask_np) > 0.8 * 255)\n",
        "\n",
        "    num_points = 10\n",
        "\n",
        "    if len(x) > 0:\n",
        "        sampled_idx = np.random.choice(len(x), min(num_points, len(x)), replace=False)\n",
        "        points = np.column_stack([x[sampled_idx], y[sampled_idx]]).astype(np.float32)\n",
        "        labels = np.ones(len(points), dtype=np.int32)\n",
        "    else:\n",
        "        points = np.empty((0, 2), dtype=np.float32)\n",
        "        labels = np.empty(0, dtype=np.int32)\n",
        "\n",
        "\n",
        "    ann_frame_idx = 0\n",
        "    ann_obj_id = 1\n",
        "    prompts[ann_obj_id] = points, labels\n",
        "\n",
        "\n",
        "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
        "        inference_state=inference_state,\n",
        "        frame_idx=ann_frame_idx,\n",
        "        obj_id=ann_obj_id,\n",
        "        points=points,\n",
        "        labels=labels,\n",
        "    )\n",
        "\n",
        "    return predictor, inference_state\n"
      ],
      "metadata": {
        "id": "ER9YWanOXLXK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Render Final SAM2 Segmented Video Output"
      ],
      "metadata": {
        "id": "8yTyAmYTXU4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def render_sam_video(video_path, video_segments, output_path, alpha=0.5):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Create color mapping for objects\n",
        "    obj_ids = list({k for frame in video_segments.values() for k in frame.keys()})\n",
        "\n",
        "    frame_idx = 0\n",
        "\n",
        "    output_dir = \"segments\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_idx in video_segments:\n",
        "            # Convert to RGB for processing (SAM masks are in RGB space)\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            overlay = frame_rgb.copy()\n",
        "\n",
        "            for obj_id, mask in video_segments[frame_idx].items():\n",
        "                # Ensure mask is 2D and matches frame dimensions\n",
        "                mask = mask[0]\n",
        "                if mask.shape != (height, width):\n",
        "                    mask = cv2.resize(mask.astype(np.uint8), (width, height))\n",
        "\n",
        "                cmap = plt.get_cmap(\"tab10\")\n",
        "\n",
        "                if(obj_id <=20):\n",
        "                  color = [255, 0, 0]\n",
        "                else:\n",
        "                  color = [0, 255, 0]\n",
        "\n",
        "                # Create colored mask\n",
        "                mask_bgr = np.zeros_like(overlay)\n",
        "                mask_bgr[mask] = color\n",
        "\n",
        "                # Blend mask with overlay\n",
        "                overlay = cv2.addWeighted(overlay, 1, mask_bgr, alpha, 0)\n",
        "\n",
        "            # Convert back to BGR for video writing\n",
        "            frame_out = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)\n",
        "            output_file = os.path.join(output_dir, f\"{frame_idx:05d}.jpg\")\n",
        "            # Save the frame as a JPG with specified quality\n",
        "            cv2.imwrite(output_file, frame_out, [int(cv2.IMWRITE_JPEG_QUALITY), 50])\n",
        "\n",
        "            out.write(frame_out)\n",
        "\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Saved SAM masked video to {output_path}\")"
      ],
      "metadata": {
        "id": "9tn10V69XOSL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DinoV2 Usage"
      ],
      "metadata": {
        "id": "j9czymraAAND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DinoV2Segmentation(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super().__init__()\n",
        "        self.dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(768, 256, kernel_size=3, padding=1),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),  # 16->32\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),  # 32->64\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),  # 64->128\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.Upsample(size=(224, 224), mode='bilinear', align_corners=True),  # 128->224\n",
        "            nn.Conv2d(32, num_classes, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get features (B, 256, 768)\n",
        "        features = self.dinov2.forward_features(x)['x_norm_patchtokens']\n",
        "\n",
        "        # Reshape to (B, 768, 16, 16)\n",
        "        features = features.permute(0, 2, 1).view(-1, 768, 16, 16)\n",
        "\n",
        "        return self.decoder(features)\n",
        "\n",
        "def predict_mask(model, image_path, threshold=0.5):\n",
        "    transform = T.Compose([\n",
        "        T.Resize((224, 224)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    original_size = image.size\n",
        "    input_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        mask = torch.sigmoid(output).squeeze().numpy()  # Remove batch/channel dim\n",
        "\n",
        "    # Threshold to get binary mask\n",
        "    binary_mask = (mask > threshold).astype(np.uint8)\n",
        "\n",
        "    # Resize mask to original image size and convert to numpy array\n",
        "    mask_img = Image.fromarray(binary_mask * 255).resize(original_size, Image.NEAREST)\n",
        "\n",
        "    return image, mask_img"
      ],
      "metadata": {
        "id": "CSUo6h8M_-u8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Code - Execution**"
      ],
      "metadata": {
        "id": "6HO8pNiKXVRu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wf-koa6NPJn",
        "outputId": "e59218dd-a0c8-4136-e1c5-ea49bac2fd66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hand Landmarking In Progress...\n",
            "Hand Landmarked Video Rendering Completed\n",
            "NO MEDIAPIPE HANDLANDMARKS WERE DETECTED. LANDMARK PIPELINE FAILURE! DINOV2 CALLING\n",
            "Video Frame Extracted\n",
            "using device: cuda\n",
            "Predictor Calling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vitb14_pretrain.pth\n",
            "100%|██████████| 330M/330M [00:01<00:00, 212MB/s]\n",
            "<ipython-input-10-60eeec585b7e>:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"dinov2_hand_trained_segmentation.pth\", map_location='cpu'))\n",
            "frame loading (JPEG): 100%|██████████| 241/241 [00:12<00:00, 18.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictor Initialized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "propagate in video: 100%|██████████| 241/241 [04:15<00:00,  1.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved SAM masked video to sam_masked_output_final.mp4\n"
          ]
        }
      ],
      "source": [
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "# Generating Hand Landmarks\n",
        "\n",
        "frame_data = process_video(\"final_corrected.mp4\", \"landmarked_output.mp4\")\n",
        "\n",
        "if(len(frame_data[0]['landmarks'])==0):\n",
        "  print(\"NO MEDIAPIPE HANDLANDMARKS WERE DETECTED. LANDMARK PIPELINE FAILURE! DINOV2 CALLING\")\n",
        "else:\n",
        "  print(\"MEDIAPIPE HANDLANDMARKS DETECTED\")\n",
        "\n",
        "print(\"Video Frame Extracted\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "    if torch.cuda.get_device_properties(0).major >= 8:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "elif device.type == \"mps\":\n",
        "    print(\n",
        "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
        "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
        "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
        "    )\n",
        "\n",
        "print(\"Predictor Calling\")\n",
        "\n",
        "model = DinoV2Segmentation(num_classes=1)\n",
        "model.load_state_dict(torch.load(\"dinov2_hand_trained_segmentation.pth\", map_location='cpu'))\n",
        "model.eval()\n",
        "\n",
        "predictor, inference_state = init_sam_predictor(model, device, sam2_checkpoint = \"sam2.1_hiera_large.pt\", model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\", video_dir = \"./frames\")\n",
        "\n",
        "print(\"Predictor Initialized\")\n",
        "\n",
        "video_segments = {}\n",
        "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
        "    video_segments[out_frame_idx] = {\n",
        "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
        "        for i, out_obj_id in enumerate(out_obj_ids)\n",
        "    }\n",
        "\n",
        "\n",
        "render_sam_video(\n",
        "    video_path=\"final_corrected.mp4\",\n",
        "    video_segments=video_segments,\n",
        "    output_path=\"sam_masked_output_final.mp4\",\n",
        "    alpha=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dx5XWGI8O4Ua"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}